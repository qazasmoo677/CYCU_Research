{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-Temporal Data Mining\n",
    "**_Dataset and Data preprocessing_**  \n",
    "*Dr. Mitra Baratchi, Leiden University*  \n",
    "*Hossein A. Rahmani, University of Zanjan*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __original_data.csv__ file includes the check-ins information by users. Each line of the files follows the following format (_tab_ separated format):  \n",
    "\n",
    "__user_ID\tPOI_ID\tcoordinate(atitude and longitude)\tcheckin_time(hour:min)\tdate_id__  \n",
    "\n",
    "p.s., check-ins made on the same date have the same date_id and there are $151589$ check-ins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this file, we will read the data and make some data preprocessing. First, we read the check-ins data file and calculate the check-in of each user and location pair with a indicator function ($1$ if a POI is checked\n",
    "by user $u$, otherwise $0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [CSV (Comma Separated Values)](https://en.wikipedia.org/wiki/Comma-separated_values) format is the most common import and export format for spreadsheets and databases.\n",
    "It is one of the interesting format for data. We import [CSV in python](https://docs.python.org/3/library/csv.html) to work with CSV files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先import會用到的庫\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db_final = client.Yelp_Final\n",
    "business_final = db_final.business\n",
    "review_final = db_final.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 建立POI對應的經緯度dict\n",
    "location_geo = {}\n",
    "bar = tqdm(total=business_final.count_documents({}), desc='Get Business Location')\n",
    "tempIds = business_final.find({}, no_cursor_timeout=True, batch_size=10)\n",
    "for item in tempIds:\n",
    "    location_geo[item['newId']] = (item['latitude'], item['longitude'])\n",
    "    bar.update(1)\n",
    "tempIds.close()\n",
    "bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立user對應的checkin數量dict\n",
    "user_checkins = {}\n",
    "bar = tqdm(total=review_final.count_documents({}), desc='Get User Checkins')\n",
    "tempIds = review_final.find({}, no_cursor_timeout=True, batch_size=10)\n",
    "for item in tempIds:\n",
    "    if (item['newUserId'], item['newBusinessId']) in user_checkins:\n",
    "        user_checkins[(item['newUserId'], item['newBusinessId'])] += 1\n",
    "    else:\n",
    "        user_checkins[(item['newUserId'], item['newBusinessId'])] = 1\n",
    "    bar.update(1)\n",
    "tempIds.close()\n",
    "bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will save the preprocessed data into two files. Here, we make a file with three columns: __User_id__, __Location_id__, and __Checkin_frequency__ for users' checkins and the second file as _geo data_ to store the location information (i.e. **Location_id**,**latitude**,**longitude**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('preprocessed_data/preprocessed_data.csv', 'w', newline='') as preprocessed_data:\n",
    "    checkins_writer = csv.writer(preprocessed_data, delimiter='\\t')\n",
    "    for checkin_info in user_checkins:\n",
    "        checkin = [checkin_info[0], checkin_info[1], user_checkins[checkin_info]]\n",
    "        checkins_writer.writerow(checkin)\n",
    "\n",
    "with open('preprocessed_data/geo_data.csv', 'w', newline='') as geo_data:\n",
    "    geo_writer = csv.writer(geo_data, delimiter='\\t')\n",
    "    for lid, geo in location_geo.items():\n",
    "        geo = [lid, geo[0], geo[1]]\n",
    "        geo_writer.writerow(geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, if you have done everything correctly, you can see the **preprocessed_data.csv** and **geo_data.csv** beside of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is used in the experiments of the following paper (originally, it is for the [Foursquare](https://foursquare.com/) location-based social network):  \n",
    "*Quan Yuan, Gao Cong, Zongyang Ma, Aixin Sun, Nadia Magnenat-Thalmann: __Time-aware point-of-interest recommendation__, SIGIR, 2013*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
