{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Def getHistory_user_lists\n",
    "def getHistory_user_lists():\n",
    "    history_u_lists = defaultdict(list)\n",
    "    history_ur_lists = defaultdict(list)\n",
    "    bar = tqdm(total=user.count_documents({}), desc='Get History_user_lists')\n",
    "    tempIds = user.find({}, no_cursor_timeout=True, batch_size=10)\n",
    "    for item in tempIds:\n",
    "        history_u_lists[int(item['newId'])] = []\n",
    "        history_ur_lists[int(item['newId'])] = []\n",
    "        for history in review.find({'user_id': item['user_id']}):\n",
    "            history_u_lists[int(item['newId'])].append(int(history['newBusinessId']))\n",
    "            history_ur_lists[int(item['newId'])].append(int(history['stars'])-1)\n",
    "        bar.update(1)\n",
    "    tempIds.close()\n",
    "    bar.close()\n",
    "    return history_u_lists, history_ur_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Def getHistory_poi_lists\n",
    "def getHistory_poi_lists():\n",
    "    history_v_lists = defaultdict(list)\n",
    "    history_vr_lists = defaultdict(list)\n",
    "    bar = tqdm(total=business.count_documents({}), desc='Get History_poi_lists')\n",
    "    tempIds = business.find({}, no_cursor_timeout=True, batch_size=10)\n",
    "    for item in tempIds:\n",
    "        history_v_lists[int(item['newId'])] = []\n",
    "        history_vr_lists[int(item['newId'])] = []\n",
    "        for history in review.find({'business_id': item['business_id']}):\n",
    "            history_v_lists[int(item['newId'])].append(int(history['newUserId']))\n",
    "            history_vr_lists[int(item['newId'])].append(int(history['stars'])-1)\n",
    "        bar.update(1)\n",
    "    tempIds.close()\n",
    "    bar.close()\n",
    "    return history_v_lists, history_vr_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Def train_test_split\n",
    "def train_test_split():\n",
    "    trainPercent = 0.8\n",
    "    matrix = dict()\n",
    "    train_u, train_v, train_r, train_s, train_t = [], [], [], [], []\n",
    "    test_u, test_v, test_r, test_s, test_t = [], [], [], [], []\n",
    "\n",
    "    # 取得全部資料\n",
    "    bar = tqdm(total=review.count_documents({}), desc='Train_Test_Split get all data')\n",
    "    tempIds = review.find({}, no_cursor_timeout=True, batch_size=10)\n",
    "    for item in tempIds:\n",
    "        tid = int(item['newUserId'])\n",
    "        if tid not in matrix:\n",
    "            matrix[tid] = []\n",
    "\n",
    "        matrix[tid].append({\n",
    "            'newBusinessId': int(item['newBusinessId']),\n",
    "            'stars': float(item['stars']),\n",
    "            'sentiment_vector': item['sentiment_vector'],\n",
    "            'timeProb': float(item['timeProb'])\n",
    "        })\n",
    "        bar.update(1)\n",
    "    tempIds.close()\n",
    "    bar.close()\n",
    "\n",
    "    for user_id, user_data in matrix.items():\n",
    "        user_data.sort(key=lambda x: x['stars'], reverse=True)\n",
    "        train_size = int(len(user_data) * trainPercent)\n",
    "        train_indices = random.sample(range(len(user_data)), train_size)\n",
    "        train_indices_set = set(train_indices)\n",
    "        \n",
    "        for i, item in enumerate(user_data):\n",
    "            if i in train_indices_set:\n",
    "                train_u.append(user_id)\n",
    "                train_v.append(item['newBusinessId'])\n",
    "                train_r.append(item['stars'])\n",
    "                train_s.append(item['sentiment_vector'])\n",
    "                train_t.append(item['timeProb'])\n",
    "            else:\n",
    "                test_u.append(user_id)\n",
    "                test_v.append(item['newBusinessId'])\n",
    "                test_r.append(item['stars'])\n",
    "                test_s.append(item['sentiment_vector'])\n",
    "                test_t.append(item['timeProb'])\n",
    "\n",
    "    return train_u, train_v, train_r, train_s, train_t, test_u, test_v, test_r, test_s, test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Def getSocial_adj_lists\n",
    "def getSocial_adj_lists():\n",
    "    social_adj_lists = defaultdict(set)\n",
    "    bar = tqdm(total=user.count_documents({}), desc='Get Social_adj_lists')\n",
    "    tempIds = user.find({}, no_cursor_timeout=True, batch_size=10)\n",
    "    for item in tempIds:\n",
    "        social_adj_lists[int(item['newId'])] = set()\n",
    "        for friend in item['newFriends']:\n",
    "            social_adj_lists[int(item['newId'])].add(int(friend))\n",
    "        bar.update(1)\n",
    "    tempIds.close()\n",
    "    bar.close()\n",
    "    return social_adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Def getSocial_adj_lists\n",
    "def getPOI_adj_lists():\n",
    "    poi_adj_lists = defaultdict(set)\n",
    "    bar = tqdm(total=business.count_documents({}), desc='Get POI_adj_lists')\n",
    "    tempIds = business.find({}, no_cursor_timeout=True, batch_size=10)\n",
    "    for item in tempIds:\n",
    "        poi_adj_lists[int(item['newId'])] = set()\n",
    "        for neighbor in item['newNeighbors']:\n",
    "            poi_adj_lists[int(item['newId'])].add(int(neighbor))\n",
    "        bar.update(1)\n",
    "    tempIds.close()\n",
    "    bar.close()\n",
    "    return poi_adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Def getRatings_list\n",
    "def getRatings_list():\n",
    "    ratings_list = dict()\n",
    "    ratings_list[1.0] = 0\n",
    "    ratings_list[2.0] = 1\n",
    "    ratings_list[3.0] = 2\n",
    "    ratings_list[4.0] = 3\n",
    "    ratings_list[5.0] = 4\n",
    "    ratings_list[1.5] = 5\n",
    "    ratings_list[2.5] = 6\n",
    "    ratings_list[3.5] = 7\n",
    "    return ratings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 設定使用的資料庫\n",
    "client = MongoClient('127.0.0.1', 27017)\n",
    "db = client.Yelp_Final\n",
    "business = db.business\n",
    "review = db.review\n",
    "user = db.user\n",
    "sentiment = db.sentiment\n",
    "\n",
    "# 儲存pkl的路徑與檔名\n",
    "dir_data = './data/final.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get History_user_lists: 100%|██████████| 1779/1779 [01:16<00:00, 23.14it/s]\n",
      "Get History_poi_lists: 100%|██████████| 11456/11456 [06:01<00:00, 31.71it/s]\n",
      "Train_Test_Split get all data: 100%|██████████| 61189/61189 [00:12<00:00, 5035.39it/s]\n",
      "Get Social_adj_lists: 100%|██████████| 1779/1779 [00:00<00:00, 35340.57it/s]\n",
      "Get POI_adj_lists: 100%|██████████| 11456/11456 [00:00<00:00, 20214.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save data to ./data/final.pickle successfully!\n"
     ]
    }
   ],
   "source": [
    "history_u_lists, history_ur_lists = getHistory_user_lists()\n",
    "history_v_lists, history_vr_lists = getHistory_poi_lists()\n",
    "train_u, train_v, train_r, train_s, train_t, test_u, test_v, test_r, test_s, test_t = train_test_split()\n",
    "social_adj_lists = getSocial_adj_lists()\n",
    "poi_adj_lists = getPOI_adj_lists()\n",
    "ratings_list = getRatings_list()\n",
    "\n",
    "data_file = open(dir_data, 'wb')\n",
    "pickle.dump((history_u_lists, history_ur_lists, history_v_lists, history_vr_lists, train_u, train_v, train_r, train_s, train_t, test_u, test_v, test_r, test_s, test_t, social_adj_lists, poi_adj_lists, ratings_list), data_file)\n",
    "data_file.close()\n",
    "print('Save data to', dir_data, 'successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = './data/final.pickle'\n",
    "data_file = open(dir_data, 'rb')\n",
    "history_u_lists, history_ur_lists, history_v_lists, history_vr_lists, train_u, train_v, train_r, train_s, train_t, test_u, test_v, test_r, test_s, test_t, social_adj_lists, POI_adj_lists, ratings_list = pickle.load(\n",
    "        data_file)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2061, 8895, 7433, 10878, 10878, 10974, 7535, 864, 3682, 9353, 4041, 2672, 1914, 8245, 9576, 3498, 8190, 9982, 9162, 6290, 5987, 8352, 8624, 2469, 8399, 3571, 5787, 221, 8322, 8389]\n"
     ]
    }
   ],
   "source": [
    "print(history_u_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668\n"
     ]
    }
   ],
   "source": [
    "k = 15\n",
    "needCalcUser = 0\n",
    "for item in history_u_lists:\n",
    "    if len(history_u_lists[item]) > k:\n",
    "        needCalcUser += 1\n",
    "print(needCalcUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_sentiment = getSentiment_list()\n",
    "data_file = open('./data/AvgSentiment.pickle', 'wb')\n",
    "pickle.dump((poi_sentiment), data_file)\n",
    "data_file.close()\n",
    "print('Save AvgSentiment successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoBERTa_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
